{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsqj8vHEAwpUJv2vThVbMF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wiritpon-code/Project-University/blob/main/%E0%B8%87%E0%B8%B2%E0%B8%99%E0%B8%81%E0%B8%A5%E0%B8%B8%E0%B9%88%E0%B8%A1%E0%B8%A7%E0%B8%B1%E0%B8%99%E0%B9%80%E0%B8%AA%E0%B8%B2%E0%B8%A3%E0%B9%8C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "นายวิริทธิ์พล ดิเรกสถาพรกุล 6704800003\n",
        "\n",
        "นายเชิดศักดิ์ สง่าอินทร์ 6704820028\n",
        "\n",
        "นายทีฆทัศน์ ทับเคลียว 6704800026"
      ],
      "metadata": {
        "id": "VFOUNd6mJBLq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23b091df"
      },
      "source": [
        "# Task\n",
        "Load the \"orders.csv\", \"items.csv\", \"products.csv\", and \"customers.csv\" files into Spark DataFrames. Then, perform a series of transformations and analyses: inspect schema and data types, create temporary views, join the DataFrames into a master DataFrame (df_master), analyze the top 5 product categories by total sales, identify the top 5 cities by number of orders, and calculate monthly sales trends. Finally, summarize the key findings from these analyses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdf6c6e0",
        "outputId": "3307e1d4-05bf-48bd-ea11-6001b4c3ae3b"
      },
      "source": [
        "print(\"Top 5 Product Categories by Total Sales:\")\n",
        "top_5_categories.show()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Product Categories by Total Sales:\n",
            "+-----------+-----------+\n",
            "|   category|total_sales|\n",
            "+-----------+-----------+\n",
            "|Electronics|      701.1|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33b4a759"
      },
      "source": [
        "## Load DataFrames\n",
        "\n",
        "### Subtask:\n",
        "Load the four CSV files (orders, items, products, and customers) into separate Spark DataFrames for initial processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cae13ea1"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize a SparkSession and then load each of the four CSV files into separate Spark DataFrames, inferring the schema and using the first row as the header, as instructed. Then display the first few rows of each DataFrame to verify the loading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "192d5d89",
        "outputId": "d64142d8-c08e-4629-a0bc-22cb720f5f80"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataLoading\").getOrCreate()\n",
        "print(\"SparkSession initialized.\")\n",
        "\n",
        "# Load orders.csv\n",
        "orders_df = spark.read.csv('orders.csv', inferSchema=True, header=True)\n",
        "print(\"Orders DataFrame loaded.\")\n",
        "orders_df.show(5)\n",
        "\n",
        "# Load items.csv\n",
        "items_df = spark.read.csv('items.csv', inferSchema=True, header=True)\n",
        "print(\"Items DataFrame loaded.\")\n",
        "items_df.show(5)\n",
        "\n",
        "# Load products.csv\n",
        "products_df = spark.read.csv('products.csv', inferSchema=True, header=True)\n",
        "print(\"Products DataFrame loaded.\")\n",
        "products_df.show(5)\n",
        "\n",
        "# Load customers.csv\n",
        "customers_df = spark.read.csv('customers.csv', inferSchema=True, header=True)\n",
        "print(\"Customers DataFrame loaded.\")\n",
        "customers_df.show(5)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkSession initialized.\n",
            "Orders DataFrame loaded.\n",
            "+--------+-----------+----------+------------+-----------+\n",
            "|order_id|customer_id|order_date|total_amount|       city|\n",
            "+--------+-----------+----------+------------+-----------+\n",
            "|       1|        101|2023-01-01|      150.75|   New York|\n",
            "|       2|        102|2023-01-02|       200.5|Los Angeles|\n",
            "|       3|        101|2023-01-03|        50.0|   New York|\n",
            "|       4|        103|2023-01-04|      300.25|    Chicago|\n",
            "|       5|        102|2023-01-05|        75.8|Los Angeles|\n",
            "+--------+-----------+----------+------------+-----------+\n",
            "\n",
            "Items DataFrame loaded.\n",
            "+-------+--------+----------+--------+-----+\n",
            "|item_id|order_id|product_id|quantity|price|\n",
            "+-------+--------+----------+--------+-----+\n",
            "|      1|       1|      1001|       1|100.0|\n",
            "|      2|       1|      1002|       2| 25.3|\n",
            "|      3|       2|      1003|       1|200.5|\n",
            "|      4|       3|      1001|       1| 50.0|\n",
            "|      5|       4|      1004|       3|100.0|\n",
            "+-------+--------+----------+--------+-----+\n",
            "\n",
            "Products DataFrame loaded.\n",
            "+----------+------------+-----------+\n",
            "|product_id|product_name|   category|\n",
            "+----------+------------+-----------+\n",
            "|      1001|      Laptop|Electronics|\n",
            "|      1002|       Mouse|Electronics|\n",
            "|      1003|    Keyboard|Electronics|\n",
            "|      1004|     Monitor|Electronics|\n",
            "|      1005|      Webcam|Accessories|\n",
            "+----------+------------+-----------+\n",
            "\n",
            "Customers DataFrame loaded.\n",
            "+-----------+-------+-------------------+-----------+\n",
            "|customer_id|   name|              email|       city|\n",
            "+-----------+-------+-------------------+-----------+\n",
            "|        101|  Alice|  alice@example.com|   New York|\n",
            "|        102|    Bob|    bob@example.com|Los Angeles|\n",
            "|        103|Charlie|charlie@example.com|    Chicago|\n",
            "|        104|  David|  david@example.com|    Houston|\n",
            "|        105|    Eve|    eve@example.com|      Miami|\n",
            "+-----------+-------+-------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf77e5f3",
        "outputId": "068b4255-624f-4194-9c92-13879647405b"
      },
      "source": [
        "orders_df.show(5)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+----------+------------+-----------+\n",
            "|order_id|customer_id|order_date|total_amount|       city|\n",
            "+--------+-----------+----------+------------+-----------+\n",
            "|       1|        101|2023-01-01|      150.75|   New York|\n",
            "|       2|        102|2023-01-02|       200.5|Los Angeles|\n",
            "|       3|        101|2023-01-03|        50.0|   New York|\n",
            "|       4|        103|2023-01-04|      300.25|    Chicago|\n",
            "|       5|        102|2023-01-05|        75.8|Los Angeles|\n",
            "+--------+-----------+----------+------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c01239f",
        "outputId": "af8904f4-2515-4323-bb37-0e0348729919"
      },
      "source": [
        "print(\"Schema for orders_df:\")\n",
        "orders_df.printSchema()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema for orders_df:\n",
            "root\n",
            " |-- order_id: integer (nullable = true)\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- order_date: date (nullable = true)\n",
            " |-- total_amount: double (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "567aade0",
        "outputId": "db11a824-7cb3-42ba-d3b4-5b9c68a0fdbf"
      },
      "source": [
        "print(\"Top 5 Product Categories by Total Sales:\")\n",
        "top_5_categories.show()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Product Categories by Total Sales:\n",
            "+-----------+-----------+\n",
            "|   category|total_sales|\n",
            "+-----------+-----------+\n",
            "|Electronics|      701.1|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "497898c8"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the CSV files were not found. I need to create dummy CSV files (`orders.csv`, `items.csv`, `products.csv`, `customers.csv`) in the current directory (`/content/`) with some example data so that Spark can load them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34c1c9ea",
        "outputId": "aa70ab35-a8ce-4452-9dcb-c66794d07f3b"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create dummy data for orders.csv\n",
        "orders_data = {\n",
        "    'order_id': [1, 2, 3, 4, 5],\n",
        "    'customer_id': [101, 102, 101, 103, 102],\n",
        "    'order_date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05'],\n",
        "    'total_amount': [150.75, 200.50, 50.00, 300.25, 75.80],\n",
        "    'city': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles']\n",
        "}\n",
        "orders_df_pd = pd.DataFrame(orders_data)\n",
        "orders_df_pd.to_csv('orders.csv', index=False)\n",
        "print(\"Created orders.csv\")\n",
        "\n",
        "# Create dummy data for items.csv\n",
        "items_data = {\n",
        "    'item_id': [1, 2, 3, 4, 5],\n",
        "    'order_id': [1, 1, 2, 3, 4],\n",
        "    'product_id': [1001, 1002, 1003, 1001, 1004],\n",
        "    'quantity': [1, 2, 1, 1, 3],\n",
        "    'price': [100.00, 25.30, 200.50, 50.00, 100.00]\n",
        "}\n",
        "items_df_pd = pd.DataFrame(items_data)\n",
        "items_df_pd.to_csv('items.csv', index=False)\n",
        "print(\"Created items.csv\")\n",
        "\n",
        "# Create dummy data for products.csv\n",
        "products_data = {\n",
        "    'product_id': [1001, 1002, 1003, 1004, 1005],\n",
        "    'product_name': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam'],\n",
        "    'category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Accessories']\n",
        "}\n",
        "products_df_pd = pd.DataFrame(products_data)\n",
        "products_df_pd.to_csv('products.csv', index=False)\n",
        "print(\"Created products.csv\")\n",
        "\n",
        "# Create dummy data for customers.csv\n",
        "customers_data = {\n",
        "    'customer_id': [101, 102, 103, 104, 105],\n",
        "    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
        "    'email': ['alice@example.com', 'bob@example.com', 'charlie@example.com', 'david@example.com', 'eve@example.com'],\n",
        "    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Miami']\n",
        "}\n",
        "customers_df_pd = pd.DataFrame(customers_data)\n",
        "customers_df_pd.to_csv('customers.csv', index=False)\n",
        "print(\"Created customers.csv\")\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"DataLoading\").getOrCreate()\n",
        "print(\"SparkSession initialized.\")\n",
        "\n",
        "# Load orders.csv\n",
        "orders_df = spark.read.csv('orders.csv', inferSchema=True, header=True)\n",
        "print(\"Orders DataFrame loaded.\")\n",
        "orders_df.show(5)\n",
        "\n",
        "# Load items.csv\n",
        "items_df = spark.read.csv('items.csv', inferSchema=True, header=True)\n",
        "print(\"Items DataFrame loaded.\")\n",
        "items_df.show(5)\n",
        "\n",
        "# Load products.csv\n",
        "products_df = spark.read.csv('products.csv', inferSchema=True, header=True)\n",
        "print(\"Products DataFrame loaded.\")\n",
        "products_df.show(5)\n",
        "\n",
        "# Load customers.csv\n",
        "customers_df = spark.read.csv('customers.csv', inferSchema=True, header=True)\n",
        "print(\"Customers DataFrame loaded.\")\n",
        "customers_df.show(5)\n"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created orders.csv\n",
            "Created items.csv\n",
            "Created products.csv\n",
            "Created customers.csv\n",
            "SparkSession initialized.\n",
            "Orders DataFrame loaded.\n",
            "+--------+-----------+----------+------------+-----------+\n",
            "|order_id|customer_id|order_date|total_amount|       city|\n",
            "+--------+-----------+----------+------------+-----------+\n",
            "|       1|        101|2023-01-01|      150.75|   New York|\n",
            "|       2|        102|2023-01-02|       200.5|Los Angeles|\n",
            "|       3|        101|2023-01-03|        50.0|   New York|\n",
            "|       4|        103|2023-01-04|      300.25|    Chicago|\n",
            "|       5|        102|2023-01-05|        75.8|Los Angeles|\n",
            "+--------+-----------+----------+------------+-----------+\n",
            "\n",
            "Items DataFrame loaded.\n",
            "+-------+--------+----------+--------+-----+\n",
            "|item_id|order_id|product_id|quantity|price|\n",
            "+-------+--------+----------+--------+-----+\n",
            "|      1|       1|      1001|       1|100.0|\n",
            "|      2|       1|      1002|       2| 25.3|\n",
            "|      3|       2|      1003|       1|200.5|\n",
            "|      4|       3|      1001|       1| 50.0|\n",
            "|      5|       4|      1004|       3|100.0|\n",
            "+-------+--------+----------+--------+-----+\n",
            "\n",
            "Products DataFrame loaded.\n",
            "+----------+------------+-----------+\n",
            "|product_id|product_name|   category|\n",
            "+----------+------------+-----------+\n",
            "|      1001|      Laptop|Electronics|\n",
            "|      1002|       Mouse|Electronics|\n",
            "|      1003|    Keyboard|Electronics|\n",
            "|      1004|     Monitor|Electronics|\n",
            "|      1005|      Webcam|Accessories|\n",
            "+----------+------------+-----------+\n",
            "\n",
            "Customers DataFrame loaded.\n",
            "+-----------+-------+-------------------+-----------+\n",
            "|customer_id|   name|              email|       city|\n",
            "+-----------+-------+-------------------+-----------+\n",
            "|        101|  Alice|  alice@example.com|   New York|\n",
            "|        102|    Bob|    bob@example.com|Los Angeles|\n",
            "|        103|Charlie|charlie@example.com|    Chicago|\n",
            "|        104|  David|  david@example.com|    Houston|\n",
            "|        105|    Eve|    eve@example.com|      Miami|\n",
            "+-----------+-------+-------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfe48d65"
      },
      "source": [
        "## Inspect Schema and Data Types\n",
        "\n",
        "### Subtask:\n",
        "Examine the schema and data types of each loaded Spark DataFrame. Perform any necessary type casting or schema adjustments to ensure data quality and compatibility for joins and analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5710169d"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires inspecting the schema of each DataFrame. I will print the schema for each of the four Spark DataFrames to verify their data types and ensure they are correctly inferred for subsequent analysis and joins. Since the `order_date` in `orders_df` was already correctly inferred as `date` type by Spark in the previous step, explicit casting is not needed at this point.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b994db34",
        "outputId": "ccc0a97d-e59c-4909-b568-281ddef88c15"
      },
      "source": [
        "print(\"Schema for orders_df:\")\n",
        "orders_df.printSchema()\n",
        "\n",
        "print(\"\\nSchema for items_df:\")\n",
        "items_df.printSchema()\n",
        "\n",
        "print(\"\\nSchema for products_df:\")\n",
        "products_df.printSchema()\n",
        "\n",
        "print(\"\\nSchema for customers_df:\")\n",
        "customers_df.printSchema()\n",
        "\n",
        "# As observed in the previous step, 'order_date' in orders_df was already inferred as DateType.\n",
        "# If it were a string, the following code would be used to cast it:\n",
        "# from pyspark.sql import functions as F\n",
        "# orders_df = orders_df.withColumn(\"order_date\", F.to_date(F.col(\"order_date\"), \"yyyy-MM-dd\"))\n",
        "# print(\"\\nSchema for orders_df after potential date casting:\")\n",
        "# orders_df.printSchema()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema for orders_df:\n",
            "root\n",
            " |-- order_id: integer (nullable = true)\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- order_date: date (nullable = true)\n",
            " |-- total_amount: double (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n",
            "\n",
            "Schema for items_df:\n",
            "root\n",
            " |-- item_id: integer (nullable = true)\n",
            " |-- order_id: integer (nullable = true)\n",
            " |-- product_id: integer (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            "\n",
            "\n",
            "Schema for products_df:\n",
            "root\n",
            " |-- product_id: integer (nullable = true)\n",
            " |-- product_name: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            "\n",
            "\n",
            "Schema for customers_df:\n",
            "root\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- email: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecb257bc"
      },
      "source": [
        "## Create Temporary Views\n",
        "\n",
        "### Subtask:\n",
        "Register each Spark DataFrame as a temporary SQL view to allow for querying using Spark SQL, as requested.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b34947b"
      },
      "source": [
        "**Reasoning**:\n",
        "To allow for querying using Spark SQL, I will register each of the four Spark DataFrames (`orders_df`, `items_df`, `products_df`, and `customers_df`) as temporary SQL views using their respective specified view names. After registering them, I will run a simple Spark SQL query to confirm that the views have been created successfully.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f28a8db8",
        "outputId": "f2aae908-1755-4f0c-aa6d-cf56a5fcc818"
      },
      "source": [
        "orders_df.createOrReplaceTempView(\"orders_view\")\n",
        "print(\"Registered orders_df as 'orders_view'\")\n",
        "\n",
        "items_df.createOrReplaceTempView(\"items_view\")\n",
        "print(\"Registered items_df as 'items_view'\")\n",
        "\n",
        "products_df.createOrReplaceTempView(\"products_view\")\n",
        "print(\"Registered products_df as 'products_view'\")\n",
        "\n",
        "customers_df.createOrReplaceTempView(\"customers_view\")\n",
        "print(\"Registered customers_df as 'customers_view'\")\n",
        "\n",
        "# Verify that the views are registered by listing them or running a simple query\n",
        "print(\"\\nVerifying temporary views:\")\n",
        "spark.sql(\"SHOW TABLES\").show()\n",
        "\n",
        "# Example query to show a few rows from one of the views\n",
        "print(\"\\nShowing data from orders_view:\")\n",
        "spark.sql(\"SELECT * FROM orders_view LIMIT 5\").show()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Registered orders_df as 'orders_view'\n",
            "Registered items_df as 'items_view'\n",
            "Registered products_df as 'products_view'\n",
            "Registered customers_df as 'customers_view'\n",
            "\n",
            "Verifying temporary views:\n",
            "+---------+--------------+-----------+\n",
            "|namespace|     tableName|isTemporary|\n",
            "+---------+--------------+-----------+\n",
            "|         |customers_view|       true|\n",
            "|         |    items_view|       true|\n",
            "|         |   orders_view|       true|\n",
            "|         | products_view|       true|\n",
            "+---------+--------------+-----------+\n",
            "\n",
            "\n",
            "Showing data from orders_view:\n",
            "+--------+-----------+----------+------------+-----------+\n",
            "|order_id|customer_id|order_date|total_amount|       city|\n",
            "+--------+-----------+----------+------------+-----------+\n",
            "|       1|        101|2023-01-01|      150.75|   New York|\n",
            "|       2|        102|2023-01-02|       200.5|Los Angeles|\n",
            "|       3|        101|2023-01-03|        50.0|   New York|\n",
            "|       4|        103|2023-01-04|      300.25|    Chicago|\n",
            "|       5|        102|2023-01-05|        75.8|Los Angeles|\n",
            "+--------+-----------+----------+------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37c14e63"
      },
      "source": [
        "## Join DataFrames\n",
        "\n",
        "### Subtask:\n",
        "Create a master DataFrame (df_master) by joining the 'orders', 'items', 'products', and 'customers' DataFrames. Ensure to join 'orders' and 'items' on 'order_id', then with 'products' on 'product_id', and finally with 'customers' on 'customer_id'. After the join, check the total number of rows to detect any unexpected increases or decreases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c56380b"
      },
      "source": [
        "**Reasoning**:\n",
        "To create the master DataFrame, I will perform a series of inner joins as specified: first between orders_df and items_df on 'order_id', then the result with products_df on 'product_id', and finally with customers_df on 'customer_id'. After all joins are complete, I will display the schema and count the rows of the final df_master DataFrame to verify the join operations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8579bc8",
        "outputId": "f0410743-0b5e-4d75-8bb7-43fe23b9b7dd"
      },
      "source": [
        "print(\"Joining orders_df and items_df...\")\n",
        "orders_items_df = orders_df.join(items_df, on='order_id', how='inner')\n",
        "print(\"Orders and Items joined. Row count: \", orders_items_df.count())\n",
        "\n",
        "print(\"Joining orders_items_df and products_df...\")\n",
        "df_master_temp = orders_items_df.join(products_df, on='product_id', how='inner')\n",
        "print(\"Orders, Items, and Products joined. Row count: \", df_master_temp.count())\n",
        "\n",
        "print(\"Joining df_master_temp and customers_df...\")\n",
        "df_master = df_master_temp.join(customers_df, on='customer_id', how='inner')\n",
        "print(\"All DataFrames joined into df_master. Row count: \", df_master.count())\n",
        "\n",
        "print(\"\\nSchema for df_master:\")\n",
        "df_master.printSchema()\n",
        "\n",
        "print(\"\\nTotal number of rows in df_master:\", df_master.count())\n",
        "\n",
        "print(\"\\nDisplaying first 5 rows of df_master:\")\n",
        "df_master.show(5)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Joining orders_df and items_df...\n",
            "Orders and Items joined. Row count:  5\n",
            "Joining orders_items_df and products_df...\n",
            "Orders, Items, and Products joined. Row count:  5\n",
            "Joining df_master_temp and customers_df...\n",
            "All DataFrames joined into df_master. Row count:  5\n",
            "\n",
            "Schema for df_master:\n",
            "root\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- product_id: integer (nullable = true)\n",
            " |-- order_id: integer (nullable = true)\n",
            " |-- order_date: date (nullable = true)\n",
            " |-- total_amount: double (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- item_id: integer (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- product_name: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- email: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            "\n",
            "\n",
            "Total number of rows in df_master: 5\n",
            "\n",
            "Displaying first 5 rows of df_master:\n",
            "+-----------+----------+--------+----------+------------+-----------+-------+--------+-----+------------+-----------+-------+-------------------+-----------+\n",
            "|customer_id|product_id|order_id|order_date|total_amount|       city|item_id|quantity|price|product_name|   category|   name|              email|       city|\n",
            "+-----------+----------+--------+----------+------------+-----------+-------+--------+-----+------------+-----------+-------+-------------------+-----------+\n",
            "|        101|      1002|       1|2023-01-01|      150.75|   New York|      2|       2| 25.3|       Mouse|Electronics|  Alice|  alice@example.com|   New York|\n",
            "|        101|      1001|       1|2023-01-01|      150.75|   New York|      1|       1|100.0|      Laptop|Electronics|  Alice|  alice@example.com|   New York|\n",
            "|        102|      1003|       2|2023-01-02|       200.5|Los Angeles|      3|       1|200.5|    Keyboard|Electronics|    Bob|    bob@example.com|Los Angeles|\n",
            "|        101|      1001|       3|2023-01-03|        50.0|   New York|      4|       1| 50.0|      Laptop|Electronics|  Alice|  alice@example.com|   New York|\n",
            "|        103|      1004|       4|2023-01-04|      300.25|    Chicago|      5|       3|100.0|     Monitor|Electronics|Charlie|charlie@example.com|    Chicago|\n",
            "+-----------+----------+--------+----------+------------+-----------+-------+--------+-----+------------+-----------+-------+-------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df5cb603"
      },
      "source": [
        "## Analyze Top Products\n",
        "\n",
        "### Subtask:\n",
        "Determine the top 5 product categories (product_category_name) with the highest total sales (price). Present the results clearly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e195bb67"
      },
      "source": [
        "**Reasoning**:\n",
        "To determine the top 5 product categories by total sales, I will calculate the 'total_sales' for each item, group the 'df_master' DataFrame by 'category', sum the 'total_sales' for each category, order the results in descending order, and then select the top 5 categories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "418a1d69",
        "outputId": "853b09ff-56d1-48ac-cf97-d0837614d517"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Calculate total sales for each item within an order\n",
        "df_master_with_sales = df_master.withColumn(\"total_sales\", F.col(\"quantity\") * F.col(\"price\"))\n",
        "\n",
        "# Group by category and sum the total sales\n",
        "top_5_categories = df_master_with_sales.groupBy(\"category\") \\\n",
        "                                       .agg(F.sum(\"total_sales\").alias(\"total_sales\")) \\\n",
        "                                       .orderBy(F.col(\"total_sales\").desc()) \\\n",
        "                                       .limit(5)\n",
        "\n",
        "print(\"Top 5 Product Categories by Total Sales:\")\n",
        "top_5_categories.show()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Product Categories by Total Sales:\n",
            "+-----------+-----------+\n",
            "|   category|total_sales|\n",
            "+-----------+-----------+\n",
            "|Electronics|      701.1|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15cdd1ae"
      },
      "source": [
        "## Analyze Top Cities\n",
        "\n",
        "### Subtask:\n",
        "Identify the top 5 cities (customer_city) with the highest number of orders. Present the results clearly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afc7c3f0"
      },
      "source": [
        "**Reasoning**:\n",
        "To identify the top 5 cities with the most orders, I will group the `df_master` DataFrame by the 'city' column that originated from the `orders_df` (the first 'city' column in the schema), count the distinct 'order_id's, order the results in descending order by the count, and then limit to the top 5 cities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64d4dbea",
        "outputId": "b88cf9eb-7323-473f-b118-3f15b6c7284c"
      },
      "source": [
        "print(\"Schema for df_master_resolved_cities:\")\n",
        "df_master_resolved_cities.printSchema()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Schema for df_master_resolved_cities:\n",
            "root\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- product_id: integer (nullable = true)\n",
            " |-- order_id: integer (nullable = true)\n",
            " |-- order_date: date (nullable = true)\n",
            " |-- total_amount: double (nullable = true)\n",
            " |-- order_city: string (nullable = true)\n",
            " |-- item_id: integer (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- product_name: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- email: string (nullable = true)\n",
            " |-- customer_city: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9161c311"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with an 'AMBIGUOUS_REFERENCE' error because the 'df_master' DataFrame contains two columns named 'city' (one from 'orders_df' and one from 'customers_df'). To resolve this, I will explicitly select the 'city' column originating from 'orders_df' by its position (index 5) in the 'df_master' schema, rename it to 'order_city' to avoid ambiguity, and then perform the aggregation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "b406d18f",
        "outputId": "7c675d03-2e6b-4a82-a962-3f7176bd7133"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# To avoid ambiguity, explicitly select the 'city' column from orders_df\n",
        "# (which is at index 5 in df_master's schema) and rename it for clarity.\n",
        "df_master_for_city_analysis = df_master.select(\n",
        "    F.col(\"order_id\"),\n",
        "    df_master.columns[5].alias(\"order_city\") # Explicitly select the 'city' column from orders_df\n",
        ")\n",
        "\n",
        "top_5_cities_by_orders = df_master_for_city_analysis.groupBy(F.col(\"order_city\")) \\\n",
        "                                  .agg(F.countDistinct(\"order_id\").alias(\"number_of_orders\")) \\\n",
        "                                  .orderBy(F.col(\"number_of_orders\").desc()) \\\n",
        "                                  .limit(5)\n",
        "\n",
        "print(\"Top 5 Cities by Number of Orders:\")\n",
        "top_5_cities_by_orders.show()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'alias'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1357850025.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m df_master_for_city_analysis = df_master.select(\n\u001b[1;32m      6\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"order_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdf_master\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"order_city\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Explicitly select the 'city' column from orders_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'alias'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0581f707"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because `df_master.columns[5]` returns a string, which does not have an `.alias()` method. To correctly rename the column, I need to convert the column name string into a Spark Column object using `F.col()` before applying the `.alias()` method. This will specifically select the 'city' column from the orders_df (at index 5 in df_master's schema) and rename it to 'order_city' for unambiguous analysis.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b78e80b"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# To avoid ambiguity, explicitly select the 'city' column from orders_df\n",
        "# (which is at index 5 in df_master's schema) and rename it for clarity.\n",
        "df_master_for_city_analysis = df_master.select(\n",
        "    F.col(\"order_id\"),\n",
        "    F.col(df_master.columns[5]).alias(\"order_city\") # Explicitly select the 'city' column from orders_df\n",
        ")\n",
        "\n",
        "top_5_cities_by_orders = df_master_for_city_analysis.groupBy(F.col(\"order_city\")) \\\n",
        "                                  .agg(F.countDistinct(\"order_id\").alias(\"number_of_orders\")) \\\n",
        "                                  .orderBy(F.col(\"number_of_orders\").desc()) \\\n",
        "                                  .limit(5)\n",
        "\n",
        "print(\"Top 5 Cities by Number of Orders:\")\n",
        "top_5_cities_by_orders.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9952fb2"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed with an 'AMBIGUOUS_REFERENCE' error because calling `F.col(\"city\")` on `df_master` is ambiguous when two columns named 'city' exist. To resolve this, I will explicitly select the 'city' column from `orders_df` (which is the first 'city' column in `df_master`'s schema) by using `df_master[\"city\"]` to refer to it, and then alias it to 'order_city' to ensure unambiguous selection for the aggregation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25b854e5"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# To avoid ambiguity, explicitly select the 'city' column from orders_df\n",
        "# (which is the first 'city' column in df_master's schema) and rename it for clarity.\n",
        "df_master_for_city_analysis = df_master.select(\n",
        "    F.col(\"order_id\"),\n",
        "    df_master[\"city\"].alias(\"order_city\") # Directly refer to the first 'city' column using dictionary-style access\n",
        ")\n",
        "\n",
        "top_5_cities_by_orders = df_master_for_city_analysis.groupBy(F.col(\"order_city\")) \\\n",
        "                                  .agg(F.countDistinct(\"order_id\").alias(\"number_of_orders\")) \\\n",
        "                                  .orderBy(F.col(\"number_of_orders\").desc()) \\\n",
        "                                  .limit(5)\n",
        "\n",
        "print(\"Top 5 Cities by Number of Orders:\")\n",
        "top_5_cities_by_orders.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84b1ad7e"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed due to an ambiguous 'city' column reference because 'df_master' contains 'city' columns from both 'orders_df' and 'customers_df'. To resolve this, I will re-perform the necessary joins within this cell, explicitly renaming the 'city' column from 'orders_df' to 'order_city' and the 'city' column from 'customers_df' to 'customer_city' before the final join. This ensures unambiguous column names for the subsequent aggregation to find the top cities by number of orders, using 'order_city' as specified.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce46d9d3"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Re-create the master DataFrame with disambiguated 'city' column names\n",
        "# This is necessary because df_master was created with ambiguous 'city' column names\n",
        "# and direct selection/renaming on an already ambiguous column is problematic.\n",
        "\n",
        "# Join orders_df and items_df\n",
        "orders_items_df = orders_df.join(items_df, on='order_id', how='inner')\n",
        "\n",
        "# Join orders_items_df and products_df\n",
        "df_master_temp = orders_items_df.join(products_df, on='product_id', how='inner')\n",
        "\n",
        "# Rename the 'city' column from orders_df (which is currently named 'city' in df_master_temp)\n",
        "# to 'order_city' to avoid ambiguity with the 'city' column from customers_df.\n",
        "# Then join with customers_df, also renaming its 'city' column to 'customer_city'.\n",
        "df_master_resolved_cities = df_master_temp.withColumnRenamed(\"city\", \"order_city\").join(\n",
        "    customers_df.withColumnRenamed(\"city\", \"customer_city\"),\n",
        "    on='customer_id',\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "# Now, perform the analysis using the disambiguated 'order_city' column\n",
        "top_5_cities_by_orders = df_master_resolved_cities.groupBy(F.col(\"order_city\")) \\\n",
        "                                  .agg(F.countDistinct(\"order_id\").alias(\"number_of_orders\")) \\\n",
        "                                  .orderBy(F.col(\"number_of_orders\").desc()) \\\n",
        "                                  .limit(5)\n",
        "\n",
        "print(\"Top 5 Cities by Number of Orders:\")\n",
        "top_5_cities_by_orders.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de4e7862"
      },
      "source": [
        "## Analyze Monthly Sales Trend\n",
        "\n",
        "### Subtask:\n",
        "Calculate and present the total sales for each month to identify any overall trends or patterns in sales over time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85ec895b"
      },
      "source": [
        "**Reasoning**:\n",
        "To analyze monthly sales trends, I will first extract the month from the 'order_date' column, calculate the 'total_sales' for each item, then group by month, sum the total sales, and finally display the results in ascending order of month.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da3d3fef"
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# 1. Extract the month from the 'order_date' column and store it in 'order_month'\n",
        "df_monthly_sales = df_master_resolved_cities.withColumn(\n",
        "    \"order_month\", F.month(F.col(\"order_date\"))\n",
        ")\n",
        "\n",
        "# 2. Calculate the 'total_sales' for each item\n",
        "df_monthly_sales = df_monthly_sales.withColumn(\n",
        "    \"item_total_sales\", F.col(\"quantity\") * F.col(\"price\")\n",
        ")\n",
        "\n",
        "# 3. Group the DataFrame by 'order_month' and aggregate the sum of 'item_total_sales'\n",
        "monthly_sales_trend = df_monthly_sales.groupBy(\"order_month\") \\\n",
        "                                      .agg(F.sum(\"item_total_sales\").alias(\"total_monthly_sales\"))\n",
        "\n",
        "# 4. Order the results by 'order_month' in ascending order\n",
        "monthly_sales_trend = monthly_sales_trend.orderBy(F.col(\"order_month\").asc())\n",
        "\n",
        "print(\"Monthly Sales Trends:\")\n",
        "# 5. Display the resulting DataFrame\n",
        "monthly_sales_trend.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29858028"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Provide a summary of the key findings from the top products, top cities, and monthly sales trend analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "058951cc"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Top Product Categories by Total Sales**: The \"Electronics\" category is identified as the top performer, generating \\$701.1 in total sales. Due to the nature of the dummy data, only one category was presented in the top results.\n",
        "*   **Top Cities by Number of Orders**: \"New York\" leads with 2 orders, followed by \"Los Angeles\" and \"Chicago\", each with 1 order. This analysis required resolving an initial ambiguity with duplicate 'city' column names by explicitly renaming them to `order_city` and `customer_city` during the DataFrame join process.\n",
        "*   **Monthly Sales Trends**: The analysis indicates that Month 1 (January) recorded total sales of \\$701.1. Given the limited dummy data, this was the only month for which sales data was available to establish a trend.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current analysis is based on a small dummy dataset. To derive more robust and meaningful insights, use a larger, more diverse dataset that covers a longer time period and a wider range of products and customers.\n",
        "*   Expand the sales trend analysis to include yearly or quarterly comparisons and incorporate additional metrics like average order value per month to identify further patterns or seasonality.\n"
      ]
    }
  ]
}